{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.linalg import circulant\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.linalg import toeplitz\n",
    "import random\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "def gaussian_random_tensor(n_components, n_features, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    components = rng.normal(loc=0.0,\n",
    "                            scale=1.0 / np.sqrt(n_components),\n",
    "                            size=(n_components, n_features))\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def hadamard_mat(n_components, n_features):\n",
    "    had = hadamard(n_features, dtype=complex)\n",
    "    had = had[np.random.choice(had.shape[0], n_components, replace=False)]\n",
    "    return had\n",
    "\n",
    "def hadamard_tensor(n_components, n_features):\n",
    "    had = hadamard(n_features)\n",
    "    had = had[np.random.choice(had.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(had, dtype = 'float32'))\n",
    "\n",
    "def diagonal_mat(n_components, n_features):\n",
    "    draw = [1, -1]\n",
    "    components = np.random.choice(a = draw, size = n_features, replace = True)\n",
    "    components = np.diag(components)\n",
    "    components = components[0:n_components]\n",
    "    components = np.sign(components)\n",
    "    return components\n",
    "\n",
    "def diagonal_tensor(n_components, n_features):\n",
    "    draw = [1, -1]\n",
    "    components = np.random.choice(a = draw, size = n_features, replace = True)\n",
    "    components = np.diag(components)\n",
    "    components = components[0:n_components]\n",
    "    components = np.sign(components)\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def circulant_random_tensor(n_components, n_features):\n",
    "    components = np.random.normal(0, 1, (1, n_features))\n",
    "    components = circulant(components)\n",
    "    diag1 = diagonal_mat(n_features, n_features)\n",
    "    # diag2 = diagonal_mat(n_features, n_features)\n",
    "    # had = hadamard_mat(n_features, n_features)\n",
    "    # HD = np.dot(had, diag1)\n",
    "    # DHD = np.dot(diag2, HD)\n",
    "    # components = np.dot(components, DHD) # Extended Ψ-regular hashing\n",
    "    components = np.dot(components, diag1) # Short Ψ-regular hashing \n",
    "    components = components[np.random.choice(components.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def toeplitz_random_tensor(n_components, n_features):\n",
    "    components = np.random.normal(0, 1, (1, n_features))\n",
    "    components = toeplitz(components)\n",
    "    diag1 = diagonal_mat(n_features, n_features)\n",
    "    diag2 = diagonal_mat(n_features, n_features)\n",
    "    had = hadamard_mat(n_features, n_features)\n",
    "    HD = np.dot(had, diag1)\n",
    "    DHD = np.dot(diag2, HD)\n",
    "    components = np.dot(components, DHD) # Extended Ψ-regular hashing\n",
    "    # components = np.dot(components, diag1) # Short Ψ-regular hashing \n",
    "    components = components[np.random.choice(components.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def kac_matrix(dim, n):\n",
    "    maxiter = n * math.log(n)\n",
    "    for k in tqdm.tqdm(range(1, int(maxiter))):\n",
    "        i = random.randint(0, n-1)\n",
    "        j = random.randint(0, n-1)\n",
    "        while j == i:\n",
    "            j = random.randint(0, n-1)\n",
    "        theta = np.random.uniform(0, 2*math.pi)\n",
    "        B_new = np.identity(n)\n",
    "        B_new[i, i] = math.sin(theta)\n",
    "        B_new[i, j] = math.cos(theta)\n",
    "        B_new[j, i] = -math.cos(theta)\n",
    "        B_new[j, j] = math.sin(theta)\n",
    "        if k == 1:\n",
    "            M = B_new\n",
    "        else:\n",
    "            M = np.dot(M, B_new)\n",
    "    M = M[np.random.choice(M.shape[0], dim, replace=False)]\n",
    "    return (tf.convert_to_tensor(M, dtype = 'float32'))\n",
    "          \n",
    "def project(x, Y):  \n",
    "    features = K.int_shape(x)[1] \n",
    "    x = tf.transpose(x)\n",
    "    # Y = tf.sign(Y) # use this if it is doing the sign BEFORE (we think right, he supposedly said wrong)\n",
    "    X_new = K.dot(Y, x)\n",
    "    X_new = tf.transpose(X_new)\n",
    "    # X_new = tf.sign(X_new) # use this if it is doing the sign AFTER (we think wrong, he says right)\n",
    "    return X_new\n",
    "\n",
    "def project_pipeline(x, R, H, D, P):\n",
    "    features = K.int_shape(x)[1] \n",
    "    x = tf.transpose(x)\n",
    "    X_new = K.dot(R, x)\n",
    "    X_new = K.dot(H, X_new)\n",
    "    X_new = K.dot(D, X_new)\n",
    "    X_new = K.dot(P, X_new)\n",
    "    X_new = tf.transpose(X_new)\n",
    "    X_new = tf.sign(X_new) # use this if it is doing the sign AFTER (we think wrong, he says right)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 333s - loss: 1.8586 - acc: 0.3302 - val_loss: 1.7429 - val_acc: 0.3744\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 333s - loss: 1.6895 - acc: 0.3968 - val_loss: 1.6606 - val_acc: 0.4081\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 325s - loss: 1.6528 - acc: 0.4110 - val_loss: 1.6605 - val_acc: 0.4037\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 332s - loss: 1.6262 - acc: 0.4218 - val_loss: 1.6665 - val_acc: 0.4070\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 326s - loss: 1.6167 - acc: 0.4224 - val_loss: 1.6473 - val_acc: 0.4096\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 327s - loss: 1.6016 - acc: 0.4304 - val_loss: 1.6891 - val_acc: 0.3935\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 321s - loss: 1.5912 - acc: 0.4326 - val_loss: 1.6371 - val_acc: 0.4165\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 316s - loss: 1.5864 - acc: 0.4368 - val_loss: 1.6292 - val_acc: 0.4227\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 317s - loss: 1.5827 - acc: 0.4373 - val_loss: 1.6221 - val_acc: 0.4230\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 317s - loss: 1.5797 - acc: 0.4381 - val_loss: 1.6136 - val_acc: 0.4244\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 297s - loss: 1.9287 - acc: 0.3083 - val_loss: 1.8128 - val_acc: 0.3555\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.7940 - acc: 0.3623 - val_loss: 1.8018 - val_acc: 0.3525\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 289s - loss: 1.7580 - acc: 0.3755 - val_loss: 1.7513 - val_acc: 0.3742\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 296s - loss: 1.7351 - acc: 0.3853 - val_loss: 1.7481 - val_acc: 0.3749\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7184 - acc: 0.3916 - val_loss: 1.7334 - val_acc: 0.3809\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7072 - acc: 0.3938 - val_loss: 1.7385 - val_acc: 0.3810\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.6945 - acc: 0.3997 - val_loss: 1.7112 - val_acc: 0.3911\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 293s - loss: 1.6859 - acc: 0.4034 - val_loss: 1.7157 - val_acc: 0.3886\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.6798 - acc: 0.4044 - val_loss: 1.7513 - val_acc: 0.3790\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.6739 - acc: 0.4054 - val_loss: 1.7173 - val_acc: 0.3902\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 289s - loss: 1.9629 - acc: 0.2887 - val_loss: 1.8600 - val_acc: 0.3332\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 291s - loss: 1.8324 - acc: 0.3430 - val_loss: 1.8240 - val_acc: 0.3437\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 293s - loss: 1.7960 - acc: 0.3570 - val_loss: 1.7918 - val_acc: 0.3624\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7704 - acc: 0.3678 - val_loss: 1.7931 - val_acc: 0.3565\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7572 - acc: 0.3708 - val_loss: 1.7723 - val_acc: 0.3682\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7431 - acc: 0.3742 - val_loss: 1.7813 - val_acc: 0.3592\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7322 - acc: 0.3818 - val_loss: 1.7806 - val_acc: 0.3656\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7242 - acc: 0.3831 - val_loss: 1.7799 - val_acc: 0.3675\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 294s - loss: 1.7169 - acc: 0.3859 - val_loss: 1.7745 - val_acc: 0.3629\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.7101 - acc: 0.3862 - val_loss: 1.7721 - val_acc: 0.3667\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 291s - loss: 2.0354 - acc: 0.2643 - val_loss: 1.9493 - val_acc: 0.3021\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.9399 - acc: 0.3065 - val_loss: 1.9290 - val_acc: 0.3187\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 289s - loss: 1.9164 - acc: 0.3164 - val_loss: 1.9061 - val_acc: 0.3169\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.8979 - acc: 0.3220 - val_loss: 1.8986 - val_acc: 0.3213\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 292s - loss: 1.8840 - acc: 0.3292 - val_loss: 1.8855 - val_acc: 0.3263\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.8729 - acc: 0.3328 - val_loss: 1.8747 - val_acc: 0.3352\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.8645 - acc: 0.3360 - val_loss: 1.8669 - val_acc: 0.3345\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.8566 - acc: 0.3380 - val_loss: 1.8698 - val_acc: 0.3343\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 291s - loss: 1.8495 - acc: 0.3404 - val_loss: 1.8713 - val_acc: 0.3327\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 290s - loss: 1.8438 - acc: 0.3419 - val_loss: 1.8720 - val_acc: 0.3319\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 296s - loss: 2.1729 - acc: 0.2022 - val_loss: 2.1321 - val_acc: 0.2201\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 289s - loss: 2.1206 - acc: 0.2286 - val_loss: 2.1135 - val_acc: 0.2292\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 289s - loss: 2.1072 - acc: 0.2334 - val_loss: 2.1132 - val_acc: 0.2341\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 290s - loss: 2.0974 - acc: 0.2381 - val_loss: 2.1037 - val_acc: 0.2381\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 290s - loss: 2.0894 - acc: 0.2416 - val_loss: 2.0931 - val_acc: 0.2385\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 289s - loss: 2.0833 - acc: 0.2453 - val_loss: 2.0908 - val_acc: 0.2420\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 289s - loss: 2.0800 - acc: 0.2471 - val_loss: 2.0881 - val_acc: 0.2424\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 291s - loss: 2.0755 - acc: 0.2467 - val_loss: 2.0866 - val_acc: 0.2455\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 290s - loss: 2.0715 - acc: 0.2481 - val_loss: 2.0903 - val_acc: 0.2436\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 290s - loss: 2.0691 - acc: 0.2479 - val_loss: 2.0849 - val_acc: 0.2469\n"
     ]
    }
   ],
   "source": [
    "dims = [1024, 512, 256, 128, 64]\n",
    "\n",
    "import time\n",
    "accuracy = []\n",
    "size = []\n",
    "times = []\n",
    "epochs = 10\n",
    "\n",
    "for d in dims:\n",
    "    start = time.time()\n",
    "    \n",
    "    if d != 1568:\n",
    "        R = diagonal_tensor(2048, 2048)\n",
    "        H = hadamard_tensor(2048, 2048)\n",
    "        D = diagonal_tensor(2048, 2048)\n",
    "        P = circulant_random_tensor(d, 2048) \n",
    "        # Y = gaussian_random_tensor(d, 1568) \n",
    "        # Y = kac_matrix(d, 1568) \n",
    "        #Y = toeplitz_random_tensor(d, 1568)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if d != 2048:\n",
    "        model.add(Lambda (lambda x: project_pipeline(x, R, H, D, P)))\n",
    "        \n",
    "    model.add(Dense(50, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    h = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "    a = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    \n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    accuracy.append(a)\n",
    "    size.append(d)\n",
    "    times.append(t)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Size': size, 'Accuracy': accuracy, 'Runtime': times})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Size</th>\n",
       "      <th>time_saved</th>\n",
       "      <th>acc_lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4244</td>\n",
       "      <td>3282.216723</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3902</td>\n",
       "      <td>2959.896222</td>\n",
       "      <td>512</td>\n",
       "      <td>0.098202</td>\n",
       "      <td>0.0342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3667</td>\n",
       "      <td>2951.960268</td>\n",
       "      <td>256</td>\n",
       "      <td>0.100620</td>\n",
       "      <td>0.0577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3319</td>\n",
       "      <td>2937.304868</td>\n",
       "      <td>128</td>\n",
       "      <td>0.105085</td>\n",
       "      <td>0.0925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2469</td>\n",
       "      <td>2935.797162</td>\n",
       "      <td>64</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.1775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy      Runtime  Size  time_saved  acc_lost\n",
       "0    0.4244  3282.216723  1024    0.000000    0.0000\n",
       "1    0.3902  2959.896222   512    0.098202    0.0342\n",
       "2    0.3667  2951.960268   256    0.100620    0.0577\n",
       "3    0.3319  2937.304868   128    0.105085    0.0925\n",
       "4    0.2469  2935.797162    64    0.105544    0.1775"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_saved'] = (max(df['Runtime']) - df['Runtime']) / max(df['Runtime'])\n",
    "df['acc_lost'] = (max(df['Accuracy']) - df['Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('Cifar_pipeline_sign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 32, 16, 16)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 32, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "lambda_25 (Lambda)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 13,904\n",
      "Trainable params: 13,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.get_layer('dense_156')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('dense_156').output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "t_sne = TSNE(n_components=2, random_state=0)\n",
    "ts = t_sne.fit_transform(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts\n",
    "\n",
    "scatter(ts, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
