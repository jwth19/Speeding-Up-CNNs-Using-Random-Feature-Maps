{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jackholder/anaconda2/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.linalg import circulant\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.linalg import toeplitz\n",
    "import random\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "def gaussian_random_tensor(n_components, n_features, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    components = rng.normal(loc=0.0,\n",
    "                            scale=1.0 / np.sqrt(n_components),\n",
    "                            size=(n_components, n_features))\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def hadamard_mat(n_components, n_features):\n",
    "    had = hadamard(n_features, dtype=complex)\n",
    "    had = had[np.random.choice(had.shape[0], n_components, replace=False)]\n",
    "    return had\n",
    "\n",
    "def diagonal_mat(n_components, n_features):\n",
    "    components = np.random.rand(n_features)\n",
    "    components = np.diag(components)\n",
    "    components = components[0:n_components]\n",
    "    components = np.sign(components)\n",
    "    return components\n",
    "\n",
    "def circulant_random_tensor(n_components, n_features):\n",
    "    components = np.random.normal(0, 1, (1, n_features))\n",
    "    components = circulant(components)\n",
    "    diag1 = diagonal_mat(n_features, n_features)\n",
    "    # diag2 = diagonal_mat(n_features, n_features)\n",
    "    # had = hadamard_mat(n_features, n_features)\n",
    "    # HD = np.dot(had, diag1)\n",
    "    # DHD = np.dot(diag2, HD)\n",
    "    # components = np.dot(components, DHD) # Extended Ψ-regular hashing\n",
    "    components = np.dot(components, diag1) # Short Ψ-regular hashing \n",
    "    components = components[np.random.choice(components.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def toeplitz_random_tensor(n_components, n_features):\n",
    "    components = np.random.normal(0, 1, (1, n_features))\n",
    "    components = toeplitz(components)\n",
    "    diag1 = diagonal_mat(n_features, n_features)\n",
    "    # diag2 = diagonal_mat(n_features, n_features)\n",
    "    # had = hadamard_mat(n_features, n_features)\n",
    "    # HD = np.dot(had, diag1)\n",
    "    # DHD = np.dot(diag2, HD)\n",
    "    # components = np.dot(components, DHD) # Extended Ψ-regular hashing\n",
    "    components = np.dot(components, diag1) # Short Ψ-regular hashing \n",
    "    components = components[np.random.choice(components.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def kac_matrix(dim, n):\n",
    "    maxiter = n * math.log(n)\n",
    "    for k in tqdm.tqdm(range(1, int(maxiter))):\n",
    "        i = random.randint(0, n-1)\n",
    "        j = random.randint(0, n-1)\n",
    "        while j == i:\n",
    "            j = random.randint(0, n-1)\n",
    "        theta = np.random.uniform(0, 2*math.pi)\n",
    "        B_new = np.identity(n)\n",
    "        B_new[i, i] = math.sin(theta)\n",
    "        B_new[i, j] = math.cos(theta)\n",
    "        B_new[j, i] = -math.cos(theta)\n",
    "        B_new[j, j] = math.sin(theta)\n",
    "        if k == 1:\n",
    "            M = B_new\n",
    "        else:\n",
    "            M = np.dot(M, B_new)\n",
    "    M = M[np.random.choice(M.shape[0], dim, replace=False)]\n",
    "    return (tf.convert_to_tensor(M, dtype = 'float32'))\n",
    "          \n",
    "def project(x, Y):  \n",
    "    features = K.int_shape(x)[1] \n",
    "    x = tf.transpose(x)\n",
    "    # Y = tf.sign(Y) # use this if it is doing the sign BEFORE (we think right, he supposedly said wrong)\n",
    "    X_new = K.dot(Y, x)\n",
    "    X_new = tf.transpose(X_new)\n",
    "    X_new = tf.sign(X_new) # use this if it is doing the sign AFTER (we think wrong, he says right)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 312s - loss: 0.1548 - acc: 0.9532 - val_loss: 0.0553 - val_acc: 0.9816\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 273s - loss: 0.0511 - acc: 0.9843 - val_loss: 0.0362 - val_acc: 0.9882\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 249s - loss: 0.0351 - acc: 0.9889 - val_loss: 0.0524 - val_acc: 0.9831\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 249s - loss: 0.0293 - acc: 0.9906 - val_loss: 0.0342 - val_acc: 0.9890\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 247s - loss: 0.0224 - acc: 0.9930 - val_loss: 0.0342 - val_acc: 0.9893\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0194 - acc: 0.9941 - val_loss: 0.0342 - val_acc: 0.9889\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0265 - val_acc: 0.9910\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0150 - acc: 0.9952 - val_loss: 0.0374 - val_acc: 0.9869\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0354 - val_acc: 0.9897\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 243s - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0322 - val_acc: 0.9897\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 250s - loss: 0.2600 - acc: 0.9188 - val_loss: 0.1618 - val_acc: 0.9482\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 248s - loss: 0.1421 - acc: 0.9545 - val_loss: 0.1456 - val_acc: 0.9533\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 251s - loss: 0.1203 - acc: 0.9612 - val_loss: 0.1468 - val_acc: 0.9542\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 249s - loss: 0.1033 - acc: 0.9663 - val_loss: 0.1624 - val_acc: 0.9505\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 248s - loss: 0.0919 - acc: 0.9700 - val_loss: 0.1362 - val_acc: 0.9593\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 248s - loss: 0.0838 - acc: 0.9725 - val_loss: 0.1485 - val_acc: 0.9574\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 248s - loss: 0.0752 - acc: 0.9744 - val_loss: 0.1687 - val_acc: 0.9523\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 248s - loss: 0.0676 - acc: 0.9770 - val_loss: 0.1649 - val_acc: 0.9537\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 248s - loss: 0.0693 - acc: 0.9767 - val_loss: 0.1450 - val_acc: 0.9557\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 249s - loss: 0.0625 - acc: 0.9791 - val_loss: 0.1619 - val_acc: 0.9514\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 243s - loss: 0.4224 - acc: 0.8649 - val_loss: 0.2895 - val_acc: 0.9095\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.2624 - acc: 0.9151 - val_loss: 0.2603 - val_acc: 0.9142\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 243s - loss: 0.2348 - acc: 0.9242 - val_loss: 0.2350 - val_acc: 0.9235\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.2177 - acc: 0.9298 - val_loss: 0.2359 - val_acc: 0.9221\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.2015 - acc: 0.9354 - val_loss: 0.2368 - val_acc: 0.9225\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 243s - loss: 0.1885 - acc: 0.9387 - val_loss: 0.2148 - val_acc: 0.9289\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.1779 - acc: 0.9427 - val_loss: 0.2205 - val_acc: 0.9308\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.1705 - acc: 0.9436 - val_loss: 0.1976 - val_acc: 0.9363\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.1617 - acc: 0.9473 - val_loss: 0.2122 - val_acc: 0.9306\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 242s - loss: 0.1555 - acc: 0.9486 - val_loss: 0.2068 - val_acc: 0.9333\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.4519 - acc: 0.8559 - val_loss: 0.2961 - val_acc: 0.9028\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.2851 - acc: 0.9081 - val_loss: 0.2517 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.2476 - acc: 0.9198 - val_loss: 0.2424 - val_acc: 0.9224\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2244 - acc: 0.9267 - val_loss: 0.2327 - val_acc: 0.9254\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.2055 - acc: 0.9333 - val_loss: 0.2197 - val_acc: 0.9294\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.1916 - acc: 0.9362 - val_loss: 0.2165 - val_acc: 0.9322\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.1804 - acc: 0.9402 - val_loss: 0.2113 - val_acc: 0.9326\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.1714 - acc: 0.9426 - val_loss: 0.2097 - val_acc: 0.9330\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.1629 - acc: 0.9459 - val_loss: 0.2070 - val_acc: 0.9336\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.1573 - acc: 0.9484 - val_loss: 0.2047 - val_acc: 0.9363\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.6822 - acc: 0.7767 - val_loss: 0.4957 - val_acc: 0.8405\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.4679 - acc: 0.8454 - val_loss: 0.4296 - val_acc: 0.8591\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.4219 - acc: 0.8590 - val_loss: 0.4396 - val_acc: 0.8556\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.3937 - acc: 0.8689 - val_loss: 0.3913 - val_acc: 0.8706\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3744 - acc: 0.8750 - val_loss: 0.3971 - val_acc: 0.8704\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 236s - loss: 0.3616 - acc: 0.8780 - val_loss: 0.3788 - val_acc: 0.8754\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.3490 - acc: 0.8824 - val_loss: 0.3711 - val_acc: 0.8787\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.3409 - acc: 0.8860 - val_loss: 0.3722 - val_acc: 0.8804\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.3331 - acc: 0.8863 - val_loss: 0.3631 - val_acc: 0.8829\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 236s - loss: 0.3271 - acc: 0.8894 - val_loss: 0.3636 - val_acc: 0.8831\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 236s - loss: 1.1685 - acc: 0.6045 - val_loss: 0.9793 - val_acc: 0.6629\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 235s - loss: 0.9942 - acc: 0.6612 - val_loss: 0.9334 - val_acc: 0.6808\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 236s - loss: 0.9488 - acc: 0.6766 - val_loss: 0.9044 - val_acc: 0.6886\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 237s - loss: 0.9169 - acc: 0.6864 - val_loss: 0.8792 - val_acc: 0.6940\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 235s - loss: 0.8932 - acc: 0.6951 - val_loss: 0.8612 - val_acc: 0.7025\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 234s - loss: 0.8766 - acc: 0.6994 - val_loss: 0.8553 - val_acc: 0.7076\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 235s - loss: 0.8625 - acc: 0.7062 - val_loss: 0.8507 - val_acc: 0.7089\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 235s - loss: 0.8535 - acc: 0.7091 - val_loss: 0.8386 - val_acc: 0.7114\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 235s - loss: 0.8449 - acc: 0.7107 - val_loss: 0.8448 - val_acc: 0.7111\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 235s - loss: 0.8382 - acc: 0.7119 - val_loss: 0.8364 - val_acc: 0.7122\n"
     ]
    }
   ],
   "source": [
    "dims = [1568, 784, 392, 196, 98, 49]\n",
    "import time\n",
    "accuracy = []\n",
    "size = []\n",
    "times = []\n",
    "epochs = 10\n",
    "\n",
    "for d in dims:\n",
    "    start = time.time()\n",
    "    \n",
    "    if d != 1568:\n",
    "        # Y = gaussian_random_tensor(d, 1568) \n",
    "        # Y = kac_matrix(d, 1568) \n",
    "        # Y = circulant_random_tensor(d, 1568) \n",
    "        Y = toeplitz_random_tensor(d, 1568)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(1,28,28), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    if d != 1568:\n",
    "        model.add(Lambda (lambda x: project(x, Y)))\n",
    "        \n",
    "    model.add(Dense(50, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    h = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=epochs, verbose=1)\n",
    "    \n",
    "    a = model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    accuracy.append(a)\n",
    "    size.append(d)\n",
    "    times.append(t)\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Size': size, 'Accuracy': accuracy, 'Runtime': times})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Size</th>\n",
       "      <th>time_saved</th>\n",
       "      <th>acc_lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9897</td>\n",
       "      <td>2573.750125</td>\n",
       "      <td>1568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9514</td>\n",
       "      <td>2514.608601</td>\n",
       "      <td>784</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.0383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9333</td>\n",
       "      <td>2449.671193</td>\n",
       "      <td>392</td>\n",
       "      <td>0.048209</td>\n",
       "      <td>0.0564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9363</td>\n",
       "      <td>2414.666112</td>\n",
       "      <td>196</td>\n",
       "      <td>0.061810</td>\n",
       "      <td>0.0534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8831</td>\n",
       "      <td>2395.492377</td>\n",
       "      <td>98</td>\n",
       "      <td>0.069260</td>\n",
       "      <td>0.1066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy      Runtime  Size  time_saved  acc_lost\n",
       "0    0.9897  2573.750125  1568    0.000000    0.0000\n",
       "1    0.9514  2514.608601   784    0.022979    0.0383\n",
       "2    0.9333  2449.671193   392    0.048209    0.0564\n",
       "3    0.9363  2414.666112   196    0.061810    0.0534\n",
       "4    0.8831  2395.492377    98    0.069260    0.1066"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_saved'] = (max(df['Runtime']) - df['Runtime']) / max(df['Runtime'])\n",
    "df['acc_lost'] = (max(df['Accuracy']) - df['Accuracy'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('Mnist_toep_sign_+baseline.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
