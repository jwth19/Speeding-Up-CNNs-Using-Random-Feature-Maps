{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.linalg import circulant\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.linalg import toeplitz\n",
    "import random\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "def gaussian_random_tensor(n_components, n_features, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    components = rng.normal(loc=0.0,\n",
    "                            scale=1.0 / np.sqrt(n_components),\n",
    "                            size=(n_components, n_features))\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def hadamard_mat(n_components, n_features):\n",
    "    had = hadamard(n_features, dtype=complex)\n",
    "    had = had[np.random.choice(had.shape[0], n_components, replace=False)]\n",
    "    return had\n",
    "\n",
    "def diagonal_mat(n_components, n_features):\n",
    "    components = np.random.rand(n_features)\n",
    "    components = np.diag(components)\n",
    "    components = components[0:n_components]\n",
    "    components = np.sign(components)\n",
    "    return components\n",
    "\n",
    "def circulant_random_tensor(n_components, n_features):\n",
    "    components = np.random.normal(0, 1, (1, n_features))\n",
    "    components = circulant(components)\n",
    "    diag1 = diagonal_mat(n_features, n_features)\n",
    "    # diag2 = diagonal_mat(n_features, n_features)\n",
    "    # had = hadamard_mat(n_features, n_features)\n",
    "    # HD = np.dot(had, diag1)\n",
    "    # DHD = np.dot(diag2, HD)\n",
    "    # components = np.dot(components, DHD) # Extended Ψ-regular hashing\n",
    "    components = np.dot(components, diag1) # Short Ψ-regular hashing \n",
    "    components = components[np.random.choice(components.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def toeplitz_random_tensor(n_components, n_features):\n",
    "    components = np.random.normal(0, 1, (1, n_features))\n",
    "    components = toeplitz(components)\n",
    "    diag1 = diagonal_mat(n_features, n_features)\n",
    "    # diag2 = diagonal_mat(n_features, n_features)\n",
    "    # had = hadamard_mat(n_features, n_features)\n",
    "    # HD = np.dot(had, diag1)\n",
    "    # DHD = np.dot(diag2, HD)\n",
    "    # components = np.dot(components, DHD) # Extended Ψ-regular hashing\n",
    "    components = np.dot(components, diag1) # Short Ψ-regular hashing \n",
    "    components = components[np.random.choice(components.shape[0], n_components, replace=False)]\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def kac_matrix(dim, n):\n",
    "    maxiter = n * math.log(n)\n",
    "    for k in tqdm.tqdm(range(1, int(maxiter))):\n",
    "        i = random.randint(0, n-1)\n",
    "        j = random.randint(0, n-1)\n",
    "        while j == i:\n",
    "            j = random.randint(0, n-1)\n",
    "        theta = np.random.uniform(0, 2*math.pi)\n",
    "        B_new = np.identity(n)\n",
    "        B_new[i, i] = math.sin(theta)\n",
    "        B_new[i, j] = math.cos(theta)\n",
    "        B_new[j, i] = -math.cos(theta)\n",
    "        B_new[j, j] = math.sin(theta)\n",
    "        if k == 1:\n",
    "            M = B_new\n",
    "        else:\n",
    "            M = np.dot(M, B_new)\n",
    "    M = M[np.random.choice(M.shape[0], dim, replace=False)]\n",
    "    return (tf.convert_to_tensor(M, dtype = 'float32'))\n",
    "          \n",
    "def project(x, Y):  \n",
    "    features = K.int_shape(x)[1] \n",
    "    x = tf.transpose(x)\n",
    "    # Y = tf.sign(Y) # use this if it is doing the sign BEFORE (we think right, he supposedly said wrong)\n",
    "    X_new = K.dot(Y, x)\n",
    "    X_new = tf.transpose(X_new)\n",
    "    X_new = tf.sign(X_new) # use this if it is doing the sign AFTER (we think wrong, he says right)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 260s - loss: 0.2406 - acc: 0.9250 - val_loss: 0.1541 - val_acc: 0.9528\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 262s - loss: 0.1203 - acc: 0.9620 - val_loss: 0.1156 - val_acc: 0.9629\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 274s - loss: 0.0931 - acc: 0.9698 - val_loss: 0.1263 - val_acc: 0.9606\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 260s - loss: 0.0749 - acc: 0.9748 - val_loss: 0.1169 - val_acc: 0.9635\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 265s - loss: 0.0625 - acc: 0.9790 - val_loss: 0.1164 - val_acc: 0.9663\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 261s - loss: 0.0512 - acc: 0.9831 - val_loss: 0.1122 - val_acc: 0.9659\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 260s - loss: 0.0462 - acc: 0.9845 - val_loss: 0.1183 - val_acc: 0.9659\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 260s - loss: 0.0410 - acc: 0.9857 - val_loss: 0.1325 - val_acc: 0.9623\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 260s - loss: 0.0391 - acc: 0.9865 - val_loss: 0.1165 - val_acc: 0.9651\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 250s - loss: 0.0352 - acc: 0.9880 - val_loss: 0.1281 - val_acc: 0.9631\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 245s - loss: 0.3057 - acc: 0.9033 - val_loss: 0.1823 - val_acc: 0.9407\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.1678 - acc: 0.9460 - val_loss: 0.1629 - val_acc: 0.9459\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.1389 - acc: 0.9549 - val_loss: 0.1501 - val_acc: 0.9488\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.1217 - acc: 0.9602 - val_loss: 0.1362 - val_acc: 0.9562\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.1063 - acc: 0.9651 - val_loss: 0.1447 - val_acc: 0.9534\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0941 - acc: 0.9693 - val_loss: 0.1378 - val_acc: 0.9552\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 243s - loss: 0.0842 - acc: 0.9718 - val_loss: 0.1581 - val_acc: 0.9522\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 243s - loss: 0.0763 - acc: 0.9743 - val_loss: 0.1471 - val_acc: 0.9542\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0684 - acc: 0.9769 - val_loss: 0.1358 - val_acc: 0.9579\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 244s - loss: 0.0640 - acc: 0.9790 - val_loss: 0.1338 - val_acc: 0.9599\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.4876 - acc: 0.8419 - val_loss: 0.3260 - val_acc: 0.8906\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.3244 - acc: 0.8948 - val_loss: 0.2958 - val_acc: 0.9003\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2866 - acc: 0.9061 - val_loss: 0.2739 - val_acc: 0.9107\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2626 - acc: 0.9147 - val_loss: 0.2682 - val_acc: 0.9102\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2448 - acc: 0.9197 - val_loss: 0.2479 - val_acc: 0.9177\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2314 - acc: 0.9242 - val_loss: 0.2498 - val_acc: 0.9166\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2209 - acc: 0.9267 - val_loss: 0.2426 - val_acc: 0.9190\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2102 - acc: 0.9299 - val_loss: 0.2557 - val_acc: 0.9172\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.2028 - acc: 0.9330 - val_loss: 0.2336 - val_acc: 0.9225\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.1955 - acc: 0.9352 - val_loss: 0.2459 - val_acc: 0.9201\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.6238 - acc: 0.7988 - val_loss: 0.4566 - val_acc: 0.8524\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.4353 - acc: 0.8568 - val_loss: 0.4111 - val_acc: 0.8647\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.3951 - acc: 0.8703 - val_loss: 0.3784 - val_acc: 0.8761\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3688 - acc: 0.8783 - val_loss: 0.3623 - val_acc: 0.8807\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3504 - acc: 0.8844 - val_loss: 0.3593 - val_acc: 0.8818\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3357 - acc: 0.8885 - val_loss: 0.3429 - val_acc: 0.8869\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3254 - acc: 0.8914 - val_loss: 0.3417 - val_acc: 0.8880\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3164 - acc: 0.8937 - val_loss: 0.3379 - val_acc: 0.8870\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.3078 - acc: 0.8977 - val_loss: 0.3424 - val_acc: 0.8894\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.3034 - acc: 0.8983 - val_loss: 0.3372 - val_acc: 0.8913\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.9787 - acc: 0.6746 - val_loss: 0.7737 - val_acc: 0.7448\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.7457 - acc: 0.7518 - val_loss: 0.7059 - val_acc: 0.7681\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 238s - loss: 0.6948 - acc: 0.7694 - val_loss: 0.6745 - val_acc: 0.7755\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.6674 - acc: 0.7773 - val_loss: 0.6551 - val_acc: 0.7785\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.6511 - acc: 0.7830 - val_loss: 0.6404 - val_acc: 0.7825\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.6379 - acc: 0.7872 - val_loss: 0.6391 - val_acc: 0.7821\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.6288 - acc: 0.7885 - val_loss: 0.6356 - val_acc: 0.7827\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 240s - loss: 0.6213 - acc: 0.7919 - val_loss: 0.6290 - val_acc: 0.7879\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.6153 - acc: 0.7931 - val_loss: 0.6327 - val_acc: 0.7883\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 239s - loss: 0.6100 - acc: 0.7938 - val_loss: 0.6204 - val_acc: 0.7888\n"
     ]
    }
   ],
   "source": [
    "dims = [784, 392, 196, 98, 49]\n",
    "import time\n",
    "accuracy = []\n",
    "size = []\n",
    "times = []\n",
    "epochs = 10\n",
    "\n",
    "for d in dims:\n",
    "    start = time.time()\n",
    "    \n",
    "    if d != 1568:\n",
    "        # Y = gaussian_random_tensor(d, 1568) \n",
    "        # Y = kac_matrix(d, 1568) \n",
    "        Y = circulant_random_tensor(d, 1568) \n",
    "        # Y = toeplitz_random_tensor(d, 1568)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(1,28,28), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    if d != 1568:\n",
    "        model.add(Lambda (lambda x: project(x, Y)))\n",
    "        \n",
    "    model.add(Dense(50, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    h = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=epochs, verbose=1)\n",
    "    \n",
    "    a = model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    accuracy.append(a)\n",
    "    size.append(d)\n",
    "    times.append(t)\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Size': size, 'Accuracy': accuracy, 'Runtime': times})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Size</th>\n",
       "      <th>time_saved</th>\n",
       "      <th>acc_lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9625</td>\n",
       "      <td>3626.306907</td>\n",
       "      <td>784</td>\n",
       "      <td>0.144657</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9533</td>\n",
       "      <td>3443.917690</td>\n",
       "      <td>392</td>\n",
       "      <td>0.187678</td>\n",
       "      <td>0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>3395.172000</td>\n",
       "      <td>196</td>\n",
       "      <td>0.199175</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8780</td>\n",
       "      <td>4051.382086</td>\n",
       "      <td>98</td>\n",
       "      <td>0.044394</td>\n",
       "      <td>0.0845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7474</td>\n",
       "      <td>4239.594226</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy      Runtime  Size  time_saved  acc_lost\n",
       "0    0.9625  3626.306907   784    0.144657    0.0000\n",
       "1    0.9533  3443.917690   392    0.187678    0.0092\n",
       "2    0.9133  3395.172000   196    0.199175    0.0492\n",
       "3    0.8780  4051.382086    98    0.044394    0.0845\n",
       "4    0.7474  4239.594226    49    0.000000    0.2151"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_saved'] = (max(df['Runtime']) - df['Runtime']) / max(df['Runtime'])\n",
    "df['acc_lost'] = (max(df['Accuracy']) - df['Accuracy'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('Mnist_kac_nosign.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
