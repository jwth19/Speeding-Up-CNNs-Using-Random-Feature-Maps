{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def gaussian_random_tensor(n_components, n_features, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    components = rng.normal(loc=0.0,\n",
    "                            scale=1.0 / np.sqrt(n_components),\n",
    "                            size=(n_components, n_features))\n",
    "    components = components.T\n",
    "    # components = np.sign(components)\n",
    "    return (tf.convert_to_tensor(components, dtype = 'float32'))\n",
    "\n",
    "def _check_density(density, n_features):\n",
    "    \"\"\"Factorize density check according to Li et al.\"\"\"\n",
    "    if density == 'auto':\n",
    "        density = 1 / np.sqrt(n_features)\n",
    "    elif density <= 0 or density > 1:\n",
    "        raise ValueError(\"Expected density in range ]0, 1], got: %r\"\n",
    "                         % density)\n",
    "    return density\n",
    "\n",
    "def sparse_random_tensor(n_components, n_features, density='auto',\n",
    "                         random_state=None):\n",
    " \n",
    "    density = _check_density(density, n_features)\n",
    "    rng = check_random_state(random_state)\n",
    "    if density == 1:\n",
    "        # skip index generation if totally dense\n",
    "        components = rng.binomial(1, 0.5, (n_components, n_features)) * 2 - 1\n",
    "        return 1 / np.sqrt(n_components) * components\n",
    "    else:\n",
    "        indices = []\n",
    "        offset = 0\n",
    "        indptr = [offset]\n",
    "        for i in range(n_components):\n",
    "            # find the indices of the non-zero components for row i\n",
    "            n_nonzero_i = rng.binomial(n_features, density)\n",
    "            indices_i = sample_without_replacement(n_features, n_nonzero_i,\n",
    "                                                   random_state=rng)\n",
    "            indices.append(indices_i)\n",
    "            offset += n_nonzero_i\n",
    "            indptr.append(offset)\n",
    "        indices = np.concatenate(indices)\n",
    "        # Among non zero components the probability of the sign is 50%/50%\n",
    "        data = rng.binomial(1, 0.5, size=np.size(indices)) * 2 - 1\n",
    "        # build the CSR structure by concatenating the rows\n",
    "        components = sp.csr_matrix((data, indices, indptr),\n",
    "                                   shape=(n_components, n_features))\n",
    "        return tf.convert_to_tensor(np.sqrt(1 / density) / np.sqrt(n_components) * components)\n",
    "\n",
    "\n",
    "def project(x, ncomp): #ncomp is the number of dimensions we want to shrink to \n",
    "    features = K.int_shape(x)[1]\n",
    "    Y = gaussian_random_tensor(ncomp, features)\n",
    "    X_new = K.dot(x, Y)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 426s - loss: 1.4002 - acc: 0.4936 - val_loss: 1.1321 - val_acc: 0.5936\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 418s - loss: 1.0184 - acc: 0.6384 - val_loss: 0.9625 - val_acc: 0.6591\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 425s - loss: 0.8324 - acc: 0.7054 - val_loss: 0.8803 - val_acc: 0.6872\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 418s - loss: 0.6960 - acc: 0.7534 - val_loss: 0.8583 - val_acc: 0.7025\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 418s - loss: 0.5707 - acc: 0.8000 - val_loss: 0.8866 - val_acc: 0.7083\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 414s - loss: 0.4624 - acc: 0.8378 - val_loss: 0.9811 - val_acc: 0.6947\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 416s - loss: 0.3813 - acc: 0.8666 - val_loss: 0.9545 - val_acc: 0.7085\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 418s - loss: 0.3099 - acc: 0.8927 - val_loss: 1.0157 - val_acc: 0.6985\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 411s - loss: 0.2746 - acc: 0.9059 - val_loss: 1.0839 - val_acc: 0.7072\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 412s - loss: 0.2535 - acc: 0.9132 - val_loss: 1.0923 - val_acc: 0.7066\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 377s - loss: 1.4484 - acc: 0.4766 - val_loss: 1.2057 - val_acc: 0.5683\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 373s - loss: 1.0868 - acc: 0.6095 - val_loss: 0.9939 - val_acc: 0.6429\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 373s - loss: 0.9084 - acc: 0.6797 - val_loss: 0.9276 - val_acc: 0.6773\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 374s - loss: 0.7863 - acc: 0.7219 - val_loss: 0.8652 - val_acc: 0.7003\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 377s - loss: 0.6855 - acc: 0.7569 - val_loss: 0.9162 - val_acc: 0.6927\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 372s - loss: 0.6015 - acc: 0.7881 - val_loss: 0.8917 - val_acc: 0.7064\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 373s - loss: 0.5179 - acc: 0.8160 - val_loss: 0.8816 - val_acc: 0.7111\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 373s - loss: 0.4546 - acc: 0.8384 - val_loss: 0.9236 - val_acc: 0.7032\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 374s - loss: 0.4070 - acc: 0.8577 - val_loss: 0.9767 - val_acc: 0.7085\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 373s - loss: 0.3691 - acc: 0.8700 - val_loss: 1.0003 - val_acc: 0.7024\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 335s - loss: 1.4934 - acc: 0.4607 - val_loss: 1.2455 - val_acc: 0.5542\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 330s - loss: 1.1050 - acc: 0.6076 - val_loss: 1.0521 - val_acc: 0.6244\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 329s - loss: 0.9400 - acc: 0.6703 - val_loss: 0.9812 - val_acc: 0.6602\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 328s - loss: 0.8091 - acc: 0.7157 - val_loss: 0.8944 - val_acc: 0.6870\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 328s - loss: 0.6998 - acc: 0.7535 - val_loss: 0.9217 - val_acc: 0.6824\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 329s - loss: 0.6111 - acc: 0.7837 - val_loss: 0.9275 - val_acc: 0.6963\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 328s - loss: 0.5401 - acc: 0.8093 - val_loss: 0.9297 - val_acc: 0.6977\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 328s - loss: 0.4749 - acc: 0.8323 - val_loss: 0.9888 - val_acc: 0.6968\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 327s - loss: 0.4122 - acc: 0.8550 - val_loss: 1.0284 - val_acc: 0.6920\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 329s - loss: 0.3702 - acc: 0.8694 - val_loss: 1.1013 - val_acc: 0.6981\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 300s - loss: 1.4469 - acc: 0.4796 - val_loss: 1.1693 - val_acc: 0.5777\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 289s - loss: 1.1278 - acc: 0.5991 - val_loss: 1.0739 - val_acc: 0.6213\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 290s - loss: 0.9776 - acc: 0.6540 - val_loss: 0.9557 - val_acc: 0.6632\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 289s - loss: 0.8731 - acc: 0.6927 - val_loss: 0.9221 - val_acc: 0.6759\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 290s - loss: 0.7869 - acc: 0.7240 - val_loss: 0.9164 - val_acc: 0.6788\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 290s - loss: 0.7121 - acc: 0.7498 - val_loss: 0.9076 - val_acc: 0.6932\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 289s - loss: 0.6469 - acc: 0.7720 - val_loss: 0.8920 - val_acc: 0.7066\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 289s - loss: 0.5816 - acc: 0.7942 - val_loss: 0.9490 - val_acc: 0.6954\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 290s - loss: 0.5298 - acc: 0.8115 - val_loss: 0.9702 - val_acc: 0.6878\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 291s - loss: 0.4892 - acc: 0.8261 - val_loss: 1.0052 - val_acc: 0.6926\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 286s - loss: 1.5288 - acc: 0.4491 - val_loss: 1.3269 - val_acc: 0.5128\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 283s - loss: 1.2360 - acc: 0.5568 - val_loss: 1.1719 - val_acc: 0.5789\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 282s - loss: 1.1100 - acc: 0.6066 - val_loss: 1.0705 - val_acc: 0.6194\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 282s - loss: 1.0283 - acc: 0.6361 - val_loss: 1.0544 - val_acc: 0.6234\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 282s - loss: 0.9640 - acc: 0.6603 - val_loss: 1.0142 - val_acc: 0.6422\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 282s - loss: 0.9098 - acc: 0.6784 - val_loss: 1.0278 - val_acc: 0.6473\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 281s - loss: 0.8636 - acc: 0.6952 - val_loss: 0.9867 - val_acc: 0.6586\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 282s - loss: 0.8212 - acc: 0.7084 - val_loss: 0.9835 - val_acc: 0.6634\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 282s - loss: 0.7829 - acc: 0.7222 - val_loss: 0.9697 - val_acc: 0.6669\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 283s - loss: 0.7500 - acc: 0.7340 - val_loss: 0.9760 - val_acc: 0.6726\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 283s - loss: 1.5856 - acc: 0.4264 - val_loss: 1.3623 - val_acc: 0.5076\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.3175 - acc: 0.5265 - val_loss: 1.2312 - val_acc: 0.5562\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.2258 - acc: 0.5621 - val_loss: 1.1517 - val_acc: 0.5833\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.1633 - acc: 0.5856 - val_loss: 1.1519 - val_acc: 0.5900\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 279s - loss: 1.1169 - acc: 0.6037 - val_loss: 1.1186 - val_acc: 0.6002\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.0786 - acc: 0.6180 - val_loss: 1.0968 - val_acc: 0.6133\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.0393 - acc: 0.6324 - val_loss: 1.0744 - val_acc: 0.6244\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.0168 - acc: 0.6403 - val_loss: 1.0452 - val_acc: 0.6326\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 278s - loss: 0.9890 - acc: 0.6502 - val_loss: 1.0390 - val_acc: 0.6309\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 280s - loss: 0.9698 - acc: 0.6538 - val_loss: 1.0354 - val_acc: 0.6423\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 282s - loss: 1.6757 - acc: 0.3885 - val_loss: 1.4546 - val_acc: 0.4762\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.4107 - acc: 0.4892 - val_loss: 1.3264 - val_acc: 0.5183\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.3222 - acc: 0.5260 - val_loss: 1.2925 - val_acc: 0.5451\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.2704 - acc: 0.5437 - val_loss: 1.2283 - val_acc: 0.5618\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.2325 - acc: 0.5601 - val_loss: 1.2139 - val_acc: 0.5694\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.2008 - acc: 0.5711 - val_loss: 1.1903 - val_acc: 0.5726\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.1786 - acc: 0.5776 - val_loss: 1.1391 - val_acc: 0.5950\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.1590 - acc: 0.5862 - val_loss: 1.1703 - val_acc: 0.5797\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 277s - loss: 1.1391 - acc: 0.5931 - val_loss: 1.1287 - val_acc: 0.6032\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 278s - loss: 1.1242 - acc: 0.5994 - val_loss: 1.1393 - val_acc: 0.5974\n"
     ]
    }
   ],
   "source": [
    "dims = [2048, 1536, 1024, 512, 256, 128, 64]\n",
    "import time\n",
    "accuracy = []\n",
    "size = []\n",
    "times = []\n",
    "epochs = 10\n",
    "\n",
    "for d in dims:\n",
    "    start = time.time()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    if d != 2048:\n",
    "        model.add(Lambda (lambda x: project(x, d)))\n",
    "    model.add(Dense(d, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    h = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "    a = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    \n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    accuracy.append(a)\n",
    "    size.append(d)\n",
    "    times.append(t)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Size': size, 'Accuracy': accuracy, 'Runtime': times})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_saved'] = (max(df['Runtime']) - df['Runtime']) / max(df['Runtime'])\n",
    "df['acc_lost'] = (max(df['Accuracy']) - df['Accuracy'])\n",
    "df.head()\n",
    "df.to_csv('Cifar1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
